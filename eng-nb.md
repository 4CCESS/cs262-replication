Engineering Notebook;

Replication:
1) Deciding the approach to replication: Some options available include a leaderless (state machine/active replication) approach, primary-backup approach, and even a multi-leader approach (scalability in mind for extra credit). Since a chat application is real-time and is expected to produce immediate message delivery, I was inclined to consider the leaderless approach. However, what is particularly concerning is the complexity that will be required to validate each replica and resolve conflicts between the states of each replica. For that reason, I opted for the primary-backup approach, specifically using synchronous updates and acknowledgements. Although this produces a bottleneck at the primary server and requires election/replacement, it is conceptually more straightforward and will ensure greater consistency among replicas, which we have decided is more important than speed. After all, the receiver doesn't know what messages they haven't gotten until they show up, but if messages are mixed up or out of order, it will be evident.

2) Discovery process: We built from the beginning with the assumption that we can take on additional servers as replicas. As such, we opted for a discovery process during initialization, rather than initializing in such a way that all three servers already know each other. This discovery process is easily repurposed for new servers and lends itself to an election process. However, we faced some challenges with a pure discovery process, so for the sake of simplicity we added two features:
Static peer list: this is a list of possible servers (peers.txt) that may be active. This allows us to perform discovery on a limited set of known options.
Roles: We created an initialization role (role=0) such that if a server is started with this role, it performs discovery until it finds two other servers on the list also in that role. When they have all pinged each other, they immediately perform an election, forming the base system. Future servers (or crashed servers being reinitialized) are started with role=3, with which they will try and find the primary/leader server. If they can find it, the leader will send an update on all data, at which point the server is upgraded to role=2, where it will be an official backup, receive write updates, and be eligible for election if the primary server stops operating. 
The discovery itself is defined in chat.proto and consists of a simple ping with id, warranting a response with id, role, and election cycle (for coordination).

3) Election: The election process is conducted by calculating a score (sum of each byte in IP address and port) and then polling each known peer for theirs. Each peer then compares the scores and determines the leader individually. The server that is actually the leader then announces itself, expecting acknowledgement. Finally, to conclude the election process, each server performs discovery again, ensuring that no stopped/failed servers are still considered.
Election Trigger 1 - Initialization: During initialization, as previously mentioned, an election is performed to select the leader.
Election Trigger 2 - Heartbeat Mechanism: Each server sends heartbeat messages to the leader on a fixed interval. If the leader fails to reply on 3 straight heartbeat messages, the server performs the election process. 
The pitfall of the current election process is that it lacks official consensus. Technically, any server, given some sort of network issue blocking the leader, may initialize the election process. Ideally, there would be some sort of consensus, under which all the backup servers agree to elect a new leader. However, we opted not to pursue this consensus to avoid complication and focus on the crash/failstop case. If, in fact, the leader fails it will affect all of the backups. A network failure would simply be an anomaly. Additionally, the client follows a process to discover the leader on its own, so it is less likely to be impacted by an anomalous server.

4) Failover: The failover process relies on quorum and elections. The server will continue functioning as properly so long as it meets quorum requirements for the number of backups available. Quorum is met when half  the number of backups or more servers (including the primary) are properly updated with write data sent by the client (registration/deletion/message send). This is important particularly for the case of a leader failure. If the backups are up to date, then when they perform an election and assume the primary role, there will be no data lost. In the case that a backup server is lost, the client can continue operating through the primary server. The existence of 3 servers ensures f=2 fault tolerance.
The data backup process relies on ReplicationUpdate rpc calls, that send all pertinent client write information, and is processed by the receiving server based on the type of write (user info, message info, etc). 

Client Changes
To account for the dynamic server environment, we made some changes to the client’s original operation to ensure continuity of service. To start, the client, rather than being hard coded with the server address, is given a list of servers (same list as peers.txt) that may house the chat service. Upon initialization, the client works through the list and attempts to connect with each server until it reaches an acknowledgement. Once a connection is established, it requests the primary/leader server information and establishes a connection to it. To ensure connectivity, we opted for a retry approach rather than a heartbeat approach. The rationale behind this is based in the belief that the servers should be consistently available for use, not reactive to attempts to use the service. In other words, It should be self-sustaining without much intervention. If the servers experience a sudden surge in requests, but are not prepared to respond, it could result in considerable time lost for all users. This is why a heartbeat protocol is appropriate for intra-server communication. However, a client may be idle for much longer, at the user’s will, and therefore does not need to have a perfect connection all the time. The retry approach will try to process a request through the primary server at most 3 times before following the original discovery process of finding the first connection and jumping to the new primary. We also made the acknowledgement system for message sending more robust, removing the bidirectional stream in favor of a send/deliver RPC. This ensures that every interaction, specifically write operations, are accounted for and correspond to updates in the primary and backup servers. 

Persistent Storage
1) Means of persistent storage: we opted for SQLite to construct the persistent storage for each individual server. We chose SQLite because of its ease of use and flexibility, given the varying types of data we initially used for our chat application (structs for users characteristics, messages, etc). We considered using a text file, but came to the conclusion that despite the small size and apparent simplicity, the parsing required would actually be more complicated. 

2) Balancing needs: In order to balance quick reads and persistent writes, we decided not to remove the unordered maps we used to store user and message information. On any write operation, we use our “persist” methods to write data into the db in addition to adding it to the temporary data structures. While the efficiency here could be debated, we are treating these data structures as a cache for persistent data. However, for complexity’s sake, it is not a true cache, given that it is 1-to-1 with the data in the DB at any given time. 

